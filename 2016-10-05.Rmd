---
title: "STA257"
author: "Neil Montgomery | HTML is official | PDF versions good for in-class use only"
date: "Last edited: `r format(Sys.time(), '%Y-%m-%d %H:%M')`"
output: 
  ioslides_presentation: 
    css: '../styles.css' 
    widescreen: true 
    transition: 0.001
---
\newcommand\given[1][]{\:#1\vert\:}
\newcommand\P[1]{P{\left(#1\right)}}

## the "negative binomial" distributions { .build }

Consider a Bernoulli$(p)$ process. Count the number of trials until the $r^{th}$ "success". 

This is a random variable. Call it $X$.

What is the p.m.f. of $X$?

$$p(k) = P(X=k) = \begin{cases}
{{k - 1} \choose {r - 1}}p^r(1-p)^{k-r}  & k\in\{r,r+1,r+2,\ldots\}\\
0 & \text{otherwise}\end{cases}$$

Is this a valid pmf? Yes (a little obscure to figure out)

We say $X$ has a negative binomial distribution with paramters $p$ and $r$, or $X \sim \text{NegBin}(p, r)$.

## added after class - I  { .small }

There was an error in what I put on the board. As penance, I'll give you some more information you might find useful in your travels, but is not so critical for this course.

First, the formula ${n \choose k} = \frac{n!}{k!(n-k)!}$ has a few conventions and extensions associated with it. That version of the formula is the most direct translation of its combinatoric interpretation (number of ways to pick $k$ out of $n$.) By convention, because of the combinatoric interpretation, when $n=0$ we say ${n \choose k} = 0$. There is "no way" to pick k items out of 0.

But that formula is often inconvenient for calculation. So often one uses (in fact we'll do this on October 12) this version:
$${n \choose k} = \frac{n(n-1)(n-2)\cdots(n-k+1)}{k!}$$

## added after class - II  { .small }

Note the numerator has $k$ terms in it. The nice thing about this formula is that it can be extended to negative numbers (and indeed to any real number, which we won't bother with) as follows:

$${-n \choose k} = \frac{(-n)(-n-1)(-n-2)\cdots(-n-k+1)}{k!}$$

By convention ${-n \choose 0}=1$. One can show (easy to find on internet) a version of the Binomial Theorem with negative exponent:

$$(1 + x)^{-n} = \sum_{k=0}^\infty {-n \choose k} x^k, \text{ for } |x| < 1$$

It is expressed this way rather than something like $(a+b)^{-n}$ to avoid difficulties in saying where the infinite series converges.

## added after class - III  { .small }

The eventual goal here is to prove the NegBin$(r,p)$ sums to 1. But that pmf has ${r - 1 \choose k -1}$ in it, which isn't exactly what appears in this "negative" Binomial theorem. The following will turn out to be useful (to follow along note there are $k$ terms on top):

$$\begin{align*}
{k + r - 1 \choose k} &= \frac{(k+r-1)(k+r-2)\cdots r}{k!}\\
&= (-1)^k\frac{(-r-(k-1))(-r-(k-2))\cdots(-r)}{k!}\\
&= (-1)^k{-r \choose k}
\end{align*}$$

Also we'll use ${k-1 \choose r-1} = {k-1 \choose k-r}$ (think combinatorically to see this is true.)

## added after class - IV { .small }

Now we'll get back to the NegBin$(r,p)$ p.m.f.:
$$\begin{align*}
\sum_{k=r}^\infty p(k) &= \sum_{k=r}^\infty {k-1 \choose r-1} p^r(1-p)^{k-r}\\
&= \sum_{k=r}^\infty {k - 1 \choose k-r} p^r(1-p)^{k-r}\\
&= \sum_{k=0}^\infty {k + r -1 \choose k} p^r(1-p)^k\\
&= p^r \sum_{k=0}^\infty (-1)^k {-r \choose k} (1-p)^k
\end{align*}$$

## added after class - V { .small }

Continued:
$$\begin{align*}
\sum_{k=r}^\infty p(k) &= p^r \sum_{k=0}^\infty (-1)^k {-r \choose k} (1-p)^k\\
&= p^r \sum_{k=0}^\infty {-r \choose k} (-1+p)^k\\
&= p^r(1 - 1 + p)^{-r} = 1
\end{align*}$$

All these developments are not hard to find in books and elsewhere. The value is in the journey more so than the final destination. We might need something from all this later on, or maybe not. You will not be responsible for it in this course.

Also, note that the (only) reason this distribution is called "negative binomial" is because of the use of the "negative" version of the Binomial Theorem.

## the "hypergeometric distributions" { .small }

Many examples from Ch. 1 (quality control, Lotto, some of the balls in urns etc.)

Consider a Bernoulli process, stopped after $n$ trials in which there were $r$ "successes". Let $X$ be the number of successes in the "first" $m$ out of $n$ trials. *Note: this relationship between hypergeometric and Bernoulli process is correct, but a bit contrived. Best to think of it in terms of sampling without replacement from finite populations. Statistics students will find they don't encounter this distribution all that often.*

What is the p.m.f. of $X$?

$$p(k) = P(X=k) = \begin{cases}
\frac{{r \choose k}{n-r \choose m-k}}{{n \choose m}} &: k \in \{0,\ldots,r\}\\
0 &: \text{ otherwise}\end{cases}$$

Is this a valid pmf? Yes. (Problem 34 from Chapter 1)

We say $X$ has a hypergeometric distribution with paramters $r$, $n$, and $m$. 

## added after class - hypergeometric vs. binomial - I { .small }

You are not responsible for this material. 

In Ch. 1 there were some urn and "quality control" questions that dealt with the as-yet unnamed hypergeometric distibutions. 

Key to the hypergeometric distributions is the notion of "sampling without replacement". 

For example, the urn has $r$ black balls and $n-r$ white balls. If you select a ball, it will be black with probability $p = r/n$ and white with $1-p = (n-r)/n$. 

Now let's say you put the ball back in the urn, and then select a ball again. "Sampling with replacement." The black/white probabilities don't change. Successive draws are *independent*. If you do this $m$ times, the total number of black balls selected will have a Binomial($m,p$) distribution.

## added after class - hypergeom vs. binom - II { .small }

(You are not responsible for this material.)

If you *don't* put the ball back in the urn after each selection you have "Sampling without replacement." Successive draws are *not* independent, because the probability of choosing a black ball in in the second selection depends on the outcome of the first selection. If you select $m$ balls the total number of black balls selected will have a hypergeometric distribution with parameters $n$, $r$, and $m$. 

Let's look at the lack of independence in more detail. If $n=100$ and $r=50$, then the chance of a black ball is $50/100$ on the first select and then either $49/99$ or $50/99$ on the second. The probability changes - lack of independence - but it doesn't change by that much really.

## added after class - hypergeom vs. binom - III { .small }

(You are not responsible for this material.)

Increase $n$ to 10000 and $r$ to 5000 (keeping $r/n$ the same.) Now the probabilities are $5000/10000$ and $4999/9999$ or $5000/9999$. Still different, but now even closer. In some sense, the lack of independence is becoming smaller.

There is a way that the hypergeometric "converges" to the binomial, if you allow $n$ and $r$ to increase in a way that $r/n$ converges to a constant. We are going to look at the following:

$$\lim_{\substack{n \to \infty \\ r \to \infty \\ r/n \to p}} 
\frac{{r \choose k}{n-r \choose m-k}}{{n \choose m}}$$

## added after class - hypergeom vs. binom - IV{ .small }

(You are not responsible for this material.)

$$
\frac{{r \choose k}{n-r \choose m-k}}{{n \choose m}} =
\frac{\frac{r(r-1) \cdots (r-k+1)}{k!}\frac{(n-r)(n-r-1) \cdots (n-r-(m-k)+1)}{(m-k)!}}{\frac{n(n-1)\cdots(n-m+1)}{m!}} \\
= \frac{m!}{k!(m-k)!} \frac{\overbrace{r(r-1) \cdots (r-k+1)}^\text{$k$ terms}
\overbrace{(n-r)(n-r-1) \cdots (n-r-(m-k)+1)}^\text{$m-k$ terms}}{\underbrace{n(n-1)\cdots(n-m+1)}_\text{$m$ terms}}\\
= {m \choose k}\underbrace{\frac{r}{n}\frac{r-1}{n-1}\cdots\frac{r-k+1}{n-k+1}}_\text{$k$ terms}\underbrace{\frac{n-r}{n-k}
\frac{n-r-1}{n-k-1}\cdots\frac{n-r-(m-k)+1}{n}}_\text{$m-k$ terms}
$$

## added after class - hypergeom vs. binom - V{ .small }

(You are not responsible for this material.)

Now let $n$ and $r$ go to $\infty$, but in proportion so that $r/n$ goes to a constant $0<p<1$. 

There is a fixed number of terms, so the limit can be applied one term at a time. The first $k$ terms convege to $r/n=p$ and the last $m-k$ terms converge to $(n-r)/n = 1-p$. So the whole thing converges to:

$${m \choose k} p^k(1-p)^{m-k}$$

## digression --- constants as "random variables"

In calculus etc. you may have (unconsciously) considered things like $f(x) + a$ for real constant $a$. This could have a few interpretations, such as:

1. The sum of the numbers $f(x)$ and $a$.
2. The value of the function $f + g$ evaluated at $x$, in which it happens that $g(x)=a$ for all $x$. 

We do this in probability as well. We can allow a random variable $X$ to be some constant $a$ no matter what. Then $X$ is discrete with :

$$\begin{align*}
p(x) &= P(X=x) = \begin{cases}
1 \ :\  x=a\\
0 \ :\ \text{otherwise,}\end{cases}\\
F(x) &= P(X\le x) =\begin{cases}
0 \ :\  x<a\\
1 \ :\ x\ge a.\end{cases}
\end{align*}$$

## the Poisson distributions - I

Named after a French guy called Poisson. 

Can be defined completely abstractly just by declaring $X$ has a Poisson distribution with parameter $\lambda$, or $X\sim$Poisson$(\lambda)$, if $X$ has p.m.f:

$$p(k) = P(X=k) = \begin{cases}
\frac{\lambda^ke^{-\lambda}}{k!} &: k \in \{0, 1, 2, \ldots\}\\
0 &: \text{otherwise.}
\end{cases}$$

Is this a valid p.m.f.? Yes. 

But this does not come close to explaining the importance of the Poisson distributions. 

## Bernoulli process --- with a time scale { .build }

Consider a Binomial$(n,p)$ distribution. Let's introduce a time scale to the underlying Bernoulli$(p)$ process.

Step 1. Take a fixed time interval $(0, t)$ and divide it into $n_1 = n$ subintervals and let $p_1 =p$. Let's say one Bernoulli$(p_1)$ trial happens inside each subinterval, and we keep track of the number of "successes".

A few "general" observations:

* Only one success can happen inside each subinterval.
* Results in non-overlapping collections of subintervals are independent.
* Successes happen at a "rate" of about $n\cdot p$ (intuitively) over the whole time $(0,t)$, and the number of $k$ successes has a Binomial$(n_1,p_1)$ distribution.
* $t$ didn't matter --- if we double it to $2t$ the rate simply doubles also, to $2np$. 

## double the number of intervals { .build }

Step 2. Divide the same interval into $n_2=2n$ subintervals. 

We want a trial to happen inside each subinterval, *but we want the same overall "rate" of success.*

So now we allow a Bernoulli($p_2$) trial with $p_2 = p/2$   to occur inside each subinterval. 

The same "general" observations continue to hold, except now the number of successes has a Binomial$(2n,p/2)$ distribution.

Step 4. Double the number of intervals again... (so now $(0,t)$ has $4n$ intervals with a Bernoulli($p/4$) trial in each)...

## pass to the limit { .build }

Define $\lambda t$ ("rate") to be fixed and always equal to $np$. This implies $p = \frac{\lambda t}{n}$.

What happens to Binomial$\left(n, \frac{\lambda t}{n}\right)$ distributions as $n \rightarrow \infty$?

$\lambda$ is the rate of occurrences per unit time.

Examples and more discussion next time.



